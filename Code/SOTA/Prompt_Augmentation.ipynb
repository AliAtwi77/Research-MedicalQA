{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87L-zQu3BXT4"
   },
   "source": [
    "# **Install Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uO5vFriTBSj6"
   },
   "outputs": [],
   "source": [
    "pip install transformers huggingface-hub accelerate hf_xet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qf02MV-tBdtn"
   },
   "source": [
    "# **Login with HuggingFace Token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYtIpctbBTRU"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"Your_HuggingFace_Token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHHKdDWrByYK"
   },
   "source": [
    "# **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g31wz1qSBTOx"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import (AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-v-f6reWB5m4"
   },
   "source": [
    "# **Setting Device to Gpu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBqkbvQrBTL-"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaNWGxVgCGqN"
   },
   "source": [
    "# **Load Emebdding Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puhav_56CKLh"
   },
   "outputs": [],
   "source": [
    "def load_clinicalbert():\n",
    "    model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5bDAtXyCLn2"
   },
   "source": [
    "# **Method to get the embedding of the question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjOrgNPECLJ3"
   },
   "outputs": [],
   "source": [
    "def embed_text(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8i4iKETMCZJl"
   },
   "source": [
    "# **Metod to Calculate Cosine Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzoqFzjfCLHT"
   },
   "outputs": [],
   "source": [
    "def cosine_sim(vec1, vec2):\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNlX9fSqD1QI"
   },
   "source": [
    "# **Normalaize Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Xj98muKD5UJ"
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    return text.strip().lower().replace(\" \", \"_\") # lowercase + replace 'space' with '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5MpPGhwThLr"
   },
   "source": [
    "# **Load Llama-3.1-8B-Instruct Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHOUvN2OVsA5"
   },
   "outputs": [],
   "source": [
    "def load_llm_pipeline():\n",
    "    model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60aWNRW2CtTz"
   },
   "source": [
    "# **Query LLM Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5lr1yvfVr-n"
   },
   "outputs": [],
   "source": [
    "def query_llm(llm_pipe, prompt):\n",
    "    result = llm_pipe(prompt, do_sample=False)[0][\"generated_text\"]\n",
    "    if \"Answer:\" in result:\n",
    "        return result.split(\"Answer:\")[-1].strip()\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usNanvpkC1aM"
   },
   "source": [
    "# **Extract Answer from Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHdqAUfGC5nz"
   },
   "outputs": [],
   "source": [
    "def extract_answer(model_output, options):\n",
    "  # Dictionary to map option letters to their corresponding index\n",
    "    choices = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
    "\n",
    "    # Use regex to find a single uppercase letter Aâ€“E with word boundaries\n",
    "    match = re.search(r\"\\b([A-E])\\b\", model_output.strip().upper()) #Removes any leading/trailing spaces, Converts the text to uppercase for case-insensitive matching\n",
    "    if match:\n",
    "        letter = match.group(1)\n",
    "        return choices[letter]\n",
    "\n",
    "    # fallback: match full option text\n",
    "    # If the model did not return a letter, this block tries to match the full option text\n",
    "    for idx, opt in enumerate(options):\n",
    "        if opt.lower() in model_output.lower():\n",
    "            return idx\n",
    "    # If No Match Found\n",
    "    return -1  # unrecognized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZBpeU2UTkIV"
   },
   "source": [
    "# **Loading Our Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2P3Wrr0TpHz"
   },
   "outputs": [],
   "source": [
    "print(\"Setting up...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer, bert_model = load_clinicalbert()\n",
    "llm_pipe = load_llm_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJpj6mScF9Or"
   },
   "source": [
    "# **Prompts and Main() with LLama Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJOYbXRZTgIx"
   },
   "source": [
    "## **1st: Normal Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YG-fbLJBSJ9n"
   },
   "outputs": [],
   "source": [
    "def build_prompt(question, options):\n",
    "    prompt = f\"\"\"\n",
    "                You are a highly knowledgeable and reliable AI assistant trained in clinical reasoning and evidence-based medicine.Your task is to analyze and answer multiple-choice questions, which require integration of clinical findings, pathophysiological understanding, and diagnostic reasoning.\n",
    "        Carefully evaluate the clinical scenario and the answer options provided. Determine the single best answer based on the information given.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Options:\n",
    "A. {options[0]}\n",
    "B. {options[1]}\n",
    "C. {options[2]}\n",
    "D. {options[3]}\n",
    "E. {options[4]}\n",
    "\n",
    "Select the single best option (A, B, C, D, or E) that most accurately answers the medical question\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sz4kTe_BUg-u"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # Load MEDQA dataset\n",
    "    with open(\"medqal.jsonl\") as f:\n",
    "        medqa_data = [json.loads(line) for line in f]\n",
    "\n",
    "    #define y_true and y_pred (used for Evaluation)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    total = 0\n",
    "\n",
    "    #looping through Medqa Questions\n",
    "    print(\"Running QA loop...\")\n",
    "    for idx, sample in enumerate(tqdm(medqa_data, desc=\"Evaluating\")):\n",
    "        question = sample[\"question\"]\n",
    "        option_dict = sample[\"options\"]  # dict: {'A': ..., 'B': ..., ...}\n",
    "        options = [option_dict[k] for k in sorted(option_dict.keys())]  # Keep order: A-E\n",
    "        correct_idx = options.index(sample[\"answer\"]) #we take it from the medqa file\n",
    "        total += 1\n",
    "\n",
    "        #Building Prompt\n",
    "        prompt = build_prompt(question, options)\n",
    "        # print(f\"Prompt:\\n{prompt}\")\n",
    "        # print('*'*10)\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        #generating output\n",
    "        output = query_llm(llm_pipe, prompt)\n",
    "        # print(output)\n",
    "\n",
    "        #extracting the answer from the generated output\n",
    "        pred_idx = extract_answer(output, options)\n",
    "\n",
    "        #append the correct answer and the predicted answer\n",
    "        y_true.append(correct_idx)\n",
    "        y_pred.append(pred_idx)\n",
    "\n",
    "        # Print answer for each question\n",
    "        print(f\"\\n--- Question {idx + 1} ---\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Options: {options}\")\n",
    "        print(f\"LLM Output: {output}\")\n",
    "        if pred_idx != -1:\n",
    "            print(f\"Predicted Answer: {chr(65 + pred_idx)}. {options[pred_idx]}\")\n",
    "        else:\n",
    "            print(\"Predicted Answer: Invalid or Unrecognized\")\n",
    "        print(f\"Correct Answer: {chr(65 + correct_idx)}. {options[correct_idx]}\")\n",
    "\n",
    "    # Evaluation (accuracy + F1_score)\n",
    "    correct = [yt == yp for yt, yp in zip(y_true, y_pred)]\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_micro = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    print(f\"Total Questions: {total}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"F1 Score (micro): {f1_micro:.2%}\")\n",
    "    print(f\"F1 Score (macro): {f1_macro:.2%}\")\n",
    "    print(f\"Invalid Predictions: {sum(p == -1 for p in y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2u0VFna5UgCk"
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSHUs5rmUIxg"
   },
   "source": [
    "## **2nd: Prompting With FewShot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyR2Y98nSJ7q"
   },
   "outputs": [],
   "source": [
    "def build_prompt(question, options):\n",
    "    few_shot_examples = [\n",
    "        {\n",
    "            \"question\": \"A 45-year-old man has had worsening fatigue and pallor. Labs show low hemoglobin. A bone marrow biopsy reveals a hypercellular marrow with increased myeloblasts. What is the most likely diagnosis?\",\n",
    "            \"options\": {\n",
    "                \"A\": \"Aplastic anemia\",\n",
    "                \"B\": \"Myelodysplastic syndrome\",\n",
    "                \"C\": \"Chronic myeloid leukemia\",\n",
    "                \"D\": \"Acute myeloid leukemia\",\n",
    "                \"E\": \"Multiple myeloma\"\n",
    "            },\n",
    "            \"answer\": \"D\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"A 29-year-old woman presents with acute onset chest pain and dyspnea. CT shows a pulmonary embolism. She recently returned from a long flight. Which of the following is the best initial treatment?\",\n",
    "            \"options\": {\n",
    "                \"A\": \"Aspirin\",\n",
    "                \"B\": \"Warfarin\",\n",
    "                \"C\": \"Low molecular weight heparin\",\n",
    "                \"D\": \"IV corticosteroids\",\n",
    "                \"E\": \"Thrombolytics\"\n",
    "            },\n",
    "            \"answer\": \"C\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Format the few-shot examples\n",
    "    formatted_examples = []\n",
    "    for example in few_shot_examples:\n",
    "        example_options = \"\\n\".join([f\"{k}. {v}\" for k, v in example[\"options\"].items()])\n",
    "        formatted_examples.append(f\"\"\"\n",
    "Question:\n",
    "{example[\"question\"]}\n",
    "\n",
    "Options:\n",
    "{example_options}\n",
    "Answer: {example[\"answer\"]}\n",
    "\"\"\")\n",
    "\n",
    "    # Join the formatted examples into a single string\n",
    "    examples_string = \"\\n\".join(formatted_examples)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert medical question-answering AI. Your task is to accurately answer the given medical question by carefully analyzing the provided question and the five multiple-choice options.\n",
    "\n",
    "{examples_string}\n",
    "---\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Options:\n",
    "A. {options[0]}\n",
    "B. {options[1]}\n",
    "C. {options[2]}\n",
    "D. {options[3]}\n",
    "E. {options[4]}\n",
    "---\n",
    "\n",
    "Based on the comprehensive information above, including the question and all five options, select the single best option (A, B, C, D, or E) that most accurately answers the medical question.\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7P-JFGr1SJ5f"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # Load MEDQA dataset\n",
    "    with open(\"medqal.jsonl\") as f:\n",
    "        medqa_data = [json.loads(line) for line in f]\n",
    "\n",
    "    #define y_true and y_pred (used for Evaluation)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    total = 0\n",
    "\n",
    "    #looping through Medqa Questions\n",
    "    print(\"Running QA loop...\")\n",
    "    for idx, sample in enumerate(tqdm(medqa_data, desc=\"Evaluating\")):\n",
    "        question = sample[\"question\"]\n",
    "        option_dict = sample[\"options\"]  # dict: {'A': ..., 'B': ..., ...}\n",
    "        options = [option_dict[k] for k in sorted(option_dict.keys())]  # Keep order: A-E\n",
    "        correct_idx = options.index(sample[\"answer\"]) #we take it from the medqa file\n",
    "        total += 1\n",
    "\n",
    "        #Building Prompt\n",
    "        prompt = build_prompt(question, options)\n",
    "        # print(f\"Prompt:\\n{prompt}\")\n",
    "        # print('*'*10)\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        #generating output\n",
    "        output = query_llm(llm_pipe, prompt)\n",
    "        # print(output)\n",
    "\n",
    "        #extracting the answer from the generated output\n",
    "        pred_idx = extract_answer(output, options)\n",
    "\n",
    "        #append the correct answer and the predicted answer\n",
    "        y_true.append(correct_idx)\n",
    "        y_pred.append(pred_idx)\n",
    "\n",
    "        # Print answer for each question\n",
    "        print(f\"\\n--- Question {idx + 1} ---\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Options: {options}\")\n",
    "        print(f\"LLM Output: {output}\")\n",
    "        if pred_idx != -1:\n",
    "            print(f\"Predicted Answer: {chr(65 + pred_idx)}. {options[pred_idx]}\")\n",
    "        else:\n",
    "            print(\"Predicted Answer: Invalid or Unrecognized\")\n",
    "        print(f\"Correct Answer: {chr(65 + correct_idx)}. {options[correct_idx]}\")\n",
    "\n",
    "    # Evaluation (accuracy + F1_score)\n",
    "    correct = [yt == yp for yt, yp in zip(y_true, y_pred)]\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_micro = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    print(f\"Total Questions: {total}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"F1 Score (micro): {f1_micro:.2%}\")\n",
    "    print(f\"F1 Score (macro): {f1_macro:.2%}\")\n",
    "    print(f\"Invalid Predictions: {sum(p == -1 for p in y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QeRE8urSJ3e"
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbH0a-2PUzHq"
   },
   "source": [
    "## **3rd: Prompting + PubMed Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWqDBvKFSJ07"
   },
   "outputs": [],
   "source": [
    "def build_prompt(question, options, top1, top2, top3, top4, top5):\n",
    "    ctx1 = \"\\n\".join(top1[2])\n",
    "    ctx2 = \"\\n\".join(top2[2])\n",
    "    ctx3 = \"\\n\".join(top3[2])\n",
    "    ctx4 = \"\\n\".join(top4[2])\n",
    "    ctx5 = \"\\n\".join(top5[2])\n",
    "    context = f\"Context from top relevant options:\\n\\nOption: {top1[0]}\\n{ctx1}\\n\\nOption: {top2[0]}\\n{ctx2}\\n\\nOption: {top3[0]}\\n{ctx3}\\n\\nOption: {top4[0]}\\n{ctx4}\\n\\nOption: {top5[0]}\\n{ctx5}\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a medical expert answering clinical board-style multiple choice questions.\n",
    "\n",
    "First, **carefully understand** the given question and all five answer options.\n",
    "\n",
    "Then, review the **contextual information** provided for the two most semantically relevant options (based on domain embeddings). Use this context as supporting evidence.\n",
    "\n",
    "Choose the **single best answer** based on both the question and the contextual support.\n",
    "\n",
    "If the context isn't enough to decide confidently, still make your best choice.\n",
    "\n",
    "---\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Options:\n",
    "A. {options[0]}\n",
    "B. {options[1]}\n",
    "C. {options[2]}\n",
    "D. {options[3]}\n",
    "E. {options[4]}\n",
    "{context}\n",
    "\n",
    "Final Answer (only one letter A/B/C/D/E, followed by the option text):\n",
    "Answer:\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e36dSC9KQrO6"
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEjRsYCESJyc"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    with open(\"mention_vectors.json\") as f:\n",
    "        mention_data = json.load(f)\n",
    "\n",
    "    normalized_mention_data = {\n",
    "        normalize(k): v for k, v in mention_data.items()\n",
    "    }\n",
    "\n",
    "    # Load MEDQA dataset\n",
    "    with open(\"medqal.jsonl\") as f:\n",
    "        medqa_data = [json.loads(line) for line in f]\n",
    "\n",
    "    #define y_true and y_pred (used for Evaluation)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    total = 0\n",
    "\n",
    "    #looping through Medqa Questions\n",
    "    print(\"Running QA loop...\")\n",
    "    for idx, sample in enumerate(tqdm(medqa_data, desc=\"Evaluating\")):\n",
    "        question = sample[\"question\"]\n",
    "        option_dict = sample[\"options\"]  # dict: {'A': ..., 'B': ..., ...}\n",
    "        options = [option_dict[k] for k in sorted(option_dict.keys())]  # Keep order: A-E\n",
    "        correct_idx = options.index(sample[\"answer\"]) #we take it from the medqa file\n",
    "        total += 1\n",
    "\n",
    "        # Embed question using your existing function\n",
    "        q_vec = embed_text(question, tokenizer, bert_model)\n",
    "\n",
    "        # Similarity scoring\n",
    "        scored = []\n",
    "        for opt in options:\n",
    "            opt_norm = normalize(opt)\n",
    "            # print(f'Normalized Option: {opt_norm}')\n",
    "            #checking for the availability of our option\n",
    "            if opt_norm not in normalized_mention_data:\n",
    "                print(f\"[WARN] Option not found in mention_data: '{opt}'\")\n",
    "                scored.append((opt, -1.0, []))\n",
    "                continue\n",
    "\n",
    "            data = normalized_mention_data[opt_norm]\n",
    "\n",
    "            # calculating the cosine similarity between question and the avg emb. vector of the options\n",
    "            sim = cosine_sim(q_vec, data[\"avg_vector\"])\n",
    "            # print(f'Similarity check between Question and Options Average Embedding:\\n Question Embedding: {q_vec}\\n Option Embedding: {data[\"avg_vector\"]}')\n",
    "            # print(f\"Shapes of the Emebddings:\")\n",
    "            # print(f\" Question Embedding Shape: {len(q_vec)}\")\n",
    "            # print(f\" Option Embedding Shape: {len(data['avg_vector'])}\\n\")\n",
    "            context = data[\"top_sentences\"][:5]\n",
    "            # add each option and it's score + context sentences\n",
    "            scored.append((opt, sim, context))\n",
    "\n",
    "        # Sort by similarity\n",
    "        top_options = sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "        #top1 and top2 are the top scoring options\n",
    "        top1, top2, top3, top4, top5 = top_options[0], top_options[1], top_options[2], top_options[3], top_options[4]\n",
    "\n",
    "        #prints used for debugging (not important)\n",
    "        # print('*'*10)\n",
    "        # print(f\"Top1: {top1}\\n\")\n",
    "        # print(f\"Top2: {top2}\\n\")\n",
    "        # print(f\"Top3: {top3}\\n\")\n",
    "        # print(f\"Top4: {top4}\\n\")\n",
    "        # print(f\"Top5: {top5}\\n\")\n",
    "        # print('*'*10)\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        #Building Prompt\n",
    "        prompt = build_prompt(question, options, top1, top2, top3, top4, top5)\n",
    "        # print(f\"Prompt:\\n{prompt}\")\n",
    "        # print('*'*10)\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        #generating output\n",
    "        output = query_llm(llm_pipe, prompt)\n",
    "        # print(output)\n",
    "        # print('*'*10)\n",
    "        #extracting the answer from the generated output\n",
    "        pred_idx = extract_answer(output, options)\n",
    "        # print(f\"Predicted Index/Answer: {pred_idx}\")\n",
    "        # print('*'*10)\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        #---------------------------------------------------------\n",
    "\n",
    "        #append the correct answer and the predicted answer\n",
    "        y_true.append(correct_idx)\n",
    "        y_pred.append(pred_idx)\n",
    "\n",
    "        # Print answer for each question\n",
    "        print(f\"\\n--- Question {idx + 1} ---\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Options: {options}\")\n",
    "        print(f\"LLM Output: {output}\")\n",
    "        if pred_idx != -1:\n",
    "            print(f\"Predicted Answer: {chr(65 + pred_idx)}. {options[pred_idx]}\")\n",
    "        else:\n",
    "            print(\"Predicted Answer: Invalid or Unrecognized\")\n",
    "        print(f\"Correct Answer: {chr(65 + correct_idx)}. {options[correct_idx]}\")\n",
    "        print('*--*'*100)\n",
    "\n",
    "    # Evaluation (accuracy + F1_score)\n",
    "    correct = [yt == yp for yt, yp in zip(y_true, y_pred)]\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_micro = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    print(f\"Total Questions: {total}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"F1 Score (micro): {f1_micro:.2%}\")\n",
    "    print(f\"F1 Score (macro): {f1_macro:.2%}\")\n",
    "    print(f\"Invalid Predictions: {sum(p == -1 for p in y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_05bmVF8SJvk"
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7fnm4M7ZJgM"
   },
   "source": [
    "## **4th: Promtping + Pubmed Context + Scoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cosGgRtzZZYP"
   },
   "outputs": [],
   "source": [
    "def build_prompt(question, options, top1, top2, top3, top4, top5):\n",
    "    ctx1 = \"\\n\".join(top1[2])\n",
    "    ctx2 = \"\\n\".join(top2[2])\n",
    "    ctx3 = \"\\n\".join(top3[2])\n",
    "    ctx4 = \"\\n\".join(top4[2])\n",
    "    ctx5 = \"\\n\".join(top5[2])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a highly specialized and precise medical diagnostic AI. Your sole objective is to identify the single, unequivocally correct answer to the provided medical multiple-choice question.\n",
    "\n",
    "---\n",
    "\n",
    "## Medical Question:\n",
    "{question}\n",
    "\n",
    "## Candidate Options:\n",
    "A. {options[0]}\n",
    "B. {options[1]}\n",
    "C. {options[2]}\n",
    "D. {options[3]}\n",
    "E. {options[4]}\n",
    "\n",
    "---\n",
    "\n",
    "## Relevance Scores:\n",
    "These scores indicate the semantic alignment between the question and each option. Use them as a preliminary guide, but **do not solely rely on them**.\n",
    "\n",
    "-   **Option '{top1[0]}'**: {top1[1]:.4f}\n",
    "-   **Option '{top2[0]}'**: {top2[1]:.4f}\n",
    "-   **Option '{top3[0]}'**: {top3[1]:.4f}\n",
    "-   **Option '{top4[0]}'**: {top4[1]:.4f}\n",
    "-   **Option '{top5[0]}'**: {top5[1]:.4f}\n",
    "\n",
    "---\n",
    "\n",
    "## Definitive Medical Evidence:\n",
    "**This section contains the critical information.** Evaluate each piece of evidence thoroughly and discern which option it directly and robustly supports or refutes. This is the primary determinant for your answer.\n",
    "\n",
    "### Evidence for Option '{top1[0]}':\n",
    "{ctx1}\n",
    "\n",
    "### Evidence for Option '{top2[0]}':\n",
    "{ctx2}\n",
    "\n",
    "### Evidence for Option '{top3[0]}':\n",
    "{ctx3}\n",
    "\n",
    "### Evidence for Option '{top4[0]}':\n",
    "{ctx4}\n",
    "\n",
    "### Evidence for Option '{top5[0]}':\n",
    "{ctx5}\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Protocol:\n",
    "1.  **Prioritize Evidence**: The correct answer is the option that is most clearly and directly supported by the **Definitive Medical Evidence**.\n",
    "2.  **Evaluate All Options**: Systematically consider each of the five options in light of *all* provided evidence.\n",
    "3.  **Identify Unambiguous Support**: Select the option for which the evidence provides the strongest, most direct, and unequivocal confirmation. If evidence for a particular option is absent or weak, it is less likely to be the correct answer.\n",
    "4.  **No Speculation**: Do not infer or speculate beyond the provided information. Stick strictly to what is directly supported.\n",
    "\n",
    "Provide only the letter (A, B, C, D, or E) corresponding to the single best and most evidence-backed option.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uw4DcimUZZVB"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    with open(\"mention_vectors.json\") as f:\n",
    "        mention_data = json.load(f)\n",
    "\n",
    "    normalized_mention_data = {\n",
    "        normalize(k): v for k, v in mention_data.items()\n",
    "    }\n",
    "\n",
    "    # Load MEDQA dataset\n",
    "    with open(\"medqal.jsonl\") as f:\n",
    "        medqa_data = [json.loads(line) for line in f]\n",
    "\n",
    "    #define y_true and y_pred (used for Evaluation)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    total = 0\n",
    "\n",
    "    #looping through Medqa Questions\n",
    "    print(\"Running QA loop...\")\n",
    "    for idx, sample in enumerate(tqdm(medqa_data, desc=\"Evaluating\")):\n",
    "        question = sample[\"question\"]\n",
    "        option_dict = sample[\"options\"]  # dict: {'A': ..., 'B': ..., ...}\n",
    "        options = [option_dict[k] for k in sorted(option_dict.keys())]  # Keep order: A-E\n",
    "        correct_idx = options.index(sample[\"answer\"]) #we take it from the medqa file\n",
    "        total += 1\n",
    "\n",
    "        # Embed question using your existing function\n",
    "        q_vec = embed_text(question, tokenizer, bert_model)\n",
    "\n",
    "        # Similarity scoring\n",
    "        scored = []\n",
    "        for opt in options:\n",
    "            opt_norm = normalize(opt)\n",
    "            print(f'Normalized Option: {opt_norm}')\n",
    "            #checking for the availability of our option\n",
    "            if opt_norm not in normalized_mention_data:\n",
    "                print(f\"[WARN] Option not found in mention_data: '{opt}'\")\n",
    "                scored.append((opt, -1.0, []))\n",
    "                continue\n",
    "\n",
    "            data = normalized_mention_data[opt_norm]\n",
    "\n",
    "            # calculating the cosine similarity between question and the avg emb. vector of the options\n",
    "            sim = cosine_sim(q_vec, data[\"avg_vector\"])\n",
    "            # print(f'Similarity check between Question and Options Average Embedding:\\n Question Embedding: {q_vec}\\n Option Embedding: {data[\"avg_vector\"]}')\n",
    "            # print(f\"Shapes of the Emebddings:\")\n",
    "            # print(f\" Question Embedding Shape: {len(q_vec)}\")\n",
    "            # print(f\" Option Embedding Shape: {len(data['avg_vector'])}\\n\")\n",
    "            context = data[\"top_sentences\"][:5]\n",
    "            # add each option and it's score + context sentences\n",
    "            scored.append((opt, sim, context))\n",
    "\n",
    "        # Sort by similarity\n",
    "        top_options = sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "        #top1 and top2 are the top scoring options\n",
    "        top1, top2, top3, top4, top5 = top_options[0], top_options[1], top_options[2], top_options[3], top_options[4]\n",
    "\n",
    "        #prints used for debugging (not important)\n",
    "        # print('*'*10)\n",
    "        # print(f\"Top1: {top1}\\n\")\n",
    "        # print(f\"Top2: {top2}\\n\")\n",
    "        # print(f\"Top3: {top3}\\n\")\n",
    "        # print(f\"Top4: {top4}\\n\")\n",
    "        # print(f\"Top5: {top5}\\n\")\n",
    "        # print('*'*10)\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        #Building Prompt\n",
    "        prompt = build_prompt(question, options, top1, top2, top3, top4, top5)\n",
    "        # print(f\"Prompt:\\n{prompt}\")\n",
    "        # print('*'*10)\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        #generating output\n",
    "        output = query_llm(llm_pipe, prompt)\n",
    "        # print(output)\n",
    "\n",
    "        #extracting the answer from the generated output\n",
    "        pred_idx = extract_answer(output, options)\n",
    "        # print(f\"Predicted Index/Answer: {pred_idx}\")\n",
    "        # print('*'*10)\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        #---------------------------------------------------------\n",
    "\n",
    "        #append the correct answer and the predicted answer\n",
    "        y_true.append(correct_idx)\n",
    "        y_pred.append(pred_idx)\n",
    "\n",
    "        # Print answer for each question\n",
    "        print(f\"\\n--- Question {idx + 1} ---\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Options: {options}\")\n",
    "        print(f\"LLM Output: {output}\")\n",
    "        if pred_idx != -1:\n",
    "            print(f\"Predicted Answer: {chr(65 + pred_idx)}. {options[pred_idx]}\")\n",
    "        else:\n",
    "            print(\"Predicted Answer: Invalid or Unrecognized\")\n",
    "        print(f\"Correct Answer: {chr(65 + correct_idx)}. {options[correct_idx]}\")\n",
    "        print('*--*'*100)\n",
    "\n",
    "    # Evaluation (accuracy + F1_score)\n",
    "    correct = [yt == yp for yt, yp in zip(y_true, y_pred)]\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_micro = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    print(f\"Total Questions: {total}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"F1 Score (micro): {f1_micro:.2%}\")\n",
    "    print(f\"F1 Score (macro): {f1_macro:.2%}\")\n",
    "    print(f\"Invalid Predictions: {sum(p == -1 for p in y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yit_opPBWpPO"
   },
   "source": [
    "## **5th: Prompting + UMLS + Pubmed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZ0JOcNfXAjW"
   },
   "outputs": [],
   "source": [
    "def build_prompt(question, options, top1, top2, top3, top4, top5):\n",
    "    ctx1 = \"\\n\".join(top1[2]) #variable used to store context retreived from our knowlegde source for the top 1 option that's embeddding is mostly similar to the question's\n",
    "    ctx2 = \"\\n\".join(top2[2]) #variable used to store context retreived from our knowlegde source for the second top option that's embeddding is mostly similar to the question's\n",
    "    ctx3 = \"\\n\".join(top3[2])\n",
    "    ctx4 = \"\\n\".join(top4[2])\n",
    "    ctx5 = \"\\n\".join(top5[2])\n",
    "    prompt = f\"\"\"\n",
    "You are an expert medical question-answering AI. Your task is to accurately answer the given medical question by carefully analyzing the provided question, the five multiple-choice options, and the supporting evidence.\n",
    "\n",
    "---\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Options:\n",
    "A. {options[0]}\n",
    "B. {options[1]}\n",
    "C. {options[2]}\n",
    "D. {options[3]}\n",
    "E. {options[4]}\n",
    "\n",
    "--- Supporting Evidence ---\n",
    "\n",
    "The following evidence is relevant to understanding the most plausible options:\n",
    "\n",
    "Evidence for Option '{top1[0]}':\n",
    "{ctx1}\n",
    "\n",
    "Evidence for Option '{top2[0]}':\n",
    "{ctx2}\n",
    "\n",
    "Evidence for Option '{top3[0]}':\n",
    "{ctx3}\n",
    "\n",
    "Evidence for Option '{top4[0]}':\n",
    "{ctx4}\n",
    "\n",
    "Evidence for Option '{top5[0]}':\n",
    "{ctx5}\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Based on the comprehensive information above, select the single best option (A, B, C, D, or E) that most accurately answers the medical question.\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5K-WS3SBXAe4"
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    return text.strip().lower().replace(\" \", \"_\") # lowercase + replace 'space' with '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGWUvollXAW2"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load mention vectors\n",
    "    with open(\"optionll_embeddings_and_sentences.json\") as f:\n",
    "        mention_data = json.load(f)\n",
    "\n",
    "    # Normalize mention keys\n",
    "    normalized_mention_data = {\n",
    "        normalize(k): v for k, v in mention_data.items()\n",
    "    }\n",
    "\n",
    "    # Load MEDQA dataset\n",
    "    with open(\"medqal.jsonl\") as f:\n",
    "        medqa_data = [json.loads(line) for line in f]\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    total = 0\n",
    "\n",
    "    print(\"Running QA loop...\")\n",
    "    for idx, sample in enumerate(tqdm(medqa_data, desc=\"Evaluating\")):\n",
    "        question = sample[\"question\"]\n",
    "        option_dict = sample[\"options\"]  # dict: {'A': ..., 'B': ..., ...}\n",
    "        options = [option_dict[k] for k in sorted(option_dict.keys())]  # Keep order: A-E\n",
    "        correct_idx = options.index(sample[\"answer\"])\n",
    "        total += 1\n",
    "\n",
    "        # Embed question using our existing function\n",
    "        q_vec = embed_text(question, tokenizer, bert_model)\n",
    "\n",
    "        # Similarity scoring\n",
    "        scored = []\n",
    "        for opt in options:\n",
    "            opt_norm = normalize(opt)\n",
    "\n",
    "            if opt_norm not in normalized_mention_data:\n",
    "                print(f\"[WARN] Option not found in mention_data: '{opt}'\")\n",
    "                scored.append((opt, -1.0, []))\n",
    "                continue\n",
    "\n",
    "            data = normalized_mention_data[opt_norm]\n",
    "            #getting the similarites between the question and the avg embedding vector of theoptions\n",
    "            sim = cosine_sim(q_vec, data[\"avg_vector\"])\n",
    "            context = data[\"top_sentences\"][:2]\n",
    "            scored.append((opt, sim, context))\n",
    "\n",
    "        # Sort by similarity\n",
    "        top_options = sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "        top1, top2, top3, top4, top5 = top_options[0], top_options[1], top_options[3], top_options[4],top_options[5]\n",
    "        # Prompt\n",
    "        prompt = build_prompt(question, options, top1, top2, top3, top4, top5)\n",
    "        #generating the llm Output\n",
    "        output = query_llm(llm_pipe, prompt)\n",
    "        #extracting the correct option  from the llm output\n",
    "        pred_idx = extract_answer(output, options)\n",
    "\n",
    "        # these prints are used for debugging\n",
    "        # print(prompt)\n",
    "        # print('*'*10)\n",
    "        # print(top1)\n",
    "        # print('*'*10)\n",
    "        # print(top2)\n",
    "        # print('*'*100)\n",
    "        # print('\\n')\n",
    "        #________________________________________________________\n",
    "\n",
    "        y_true.append(correct_idx)\n",
    "        y_pred.append(pred_idx)\n",
    "\n",
    "        # Print answer for each question\n",
    "        print(f\"\\n--- Question {idx + 1} ---\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Options: {options}\")\n",
    "        print(f\"LLM Output: {output}\")\n",
    "        if pred_idx != -1:\n",
    "            print(f\"Predicted Answer: {chr(65 + pred_idx)}. {options[pred_idx]}\")\n",
    "        else:\n",
    "            print(\"Predicted Answer: Invalid or Unrecognized\")\n",
    "        print(f\"Correct Answer: {chr(65 + correct_idx)}. {options[correct_idx]}\")\n",
    "        print('-'*100)\n",
    "\n",
    "    # Evaluation\n",
    "    correct = [yt == yp for yt, yp in zip(y_true, y_pred)]\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_micro = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    print(f\"Total Questions: {total}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"F1 Score (micro): {f1_micro:.2%}\")\n",
    "    print(f\"F1 Score (macro): {f1_macro:.2%}\")\n",
    "    print(f\"Top-1 Hits: {sum(correct)} / {total}\")\n",
    "    print(f\"Invalid Predictions: {sum(p == -1 for p in y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XW2xYQ-DVTvo"
   },
   "source": [
    "## **6th: Prompting + UMLS + Pubmed + Scoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoLidfcQVS_R"
   },
   "outputs": [],
   "source": [
    "def build_prompt(question, options, top1, top2, top3, top4, top5):\n",
    "    ctx1 = \"\\n\".join(top1[2])\n",
    "    ctx2 = \"\\n\".join(top2[2])\n",
    "    ctx3 = \"\\n\".join(top3[2])\n",
    "    ctx4 = \"\\n\".join(top4[2])\n",
    "    ctx5 = \"\\n\".join(top5[2])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a highly specialized and precise medical diagnostic AI. Your sole objective is to identify the single, unequivocally correct answer to the provided medical multiple-choice question.\n",
    "\n",
    "---\n",
    "\n",
    "## Medical Question:\n",
    "{question}\n",
    "\n",
    "## Candidate Options:\n",
    "A. {options[0]}\n",
    "B. {options[1]}\n",
    "C. {options[2]}\n",
    "D. {options[3]}\n",
    "E. {options[4]}\n",
    "\n",
    "---\n",
    "\n",
    "## Relevance Scores:\n",
    "These scores indicate the semantic alignment between the question and each option. Use them as a preliminary guide, but **do not solely rely on them**.\n",
    "\n",
    "-   **Option '{top1[0]}'**: {top1[1]:.4f}\n",
    "-   **Option '{top2[0]}'**: {top2[1]:.4f}\n",
    "-   **Option '{top3[0]}'**: {top3[1]:.4f}\n",
    "-   **Option '{top4[0]}'**: {top4[1]:.4f}\n",
    "-   **Option '{top5[0]}'**: {top5[1]:.4f}\n",
    "\n",
    "---\n",
    "\n",
    "## Definitive Medical Evidence:\n",
    "**This section contains the critical information.** Evaluate each piece of evidence thoroughly and discern which option it directly and robustly supports or refutes. This is the primary determinant for your answer.\n",
    "\n",
    "### Evidence for Option '{top1[0]}':\n",
    "{ctx1}\n",
    "\n",
    "### Evidence for Option '{top2[0]}':\n",
    "{ctx2}\n",
    "\n",
    "### Evidence for Option '{top3[0]}':\n",
    "{ctx3}\n",
    "\n",
    "### Evidence for Option '{top4[0]}':\n",
    "{ctx4}\n",
    "\n",
    "### Evidence for Option '{top5[0]}':\n",
    "{ctx5}\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Protocol:\n",
    "1.  **Prioritize Evidence**: The correct answer is the option that is most clearly and directly supported by the **Definitive Medical Evidence**.\n",
    "2.  **Evaluate All Options**: Systematically consider each of the five options in light of *all* provided evidence.\n",
    "3.  **Identify Unambiguous Support**: Select the option for which the evidence provides the strongest, most direct, and unequivocal confirmation. If evidence for a particular option is absent or weak, it is less likely to be the correct answer.\n",
    "4.  **No Speculation**: Do not infer or speculate beyond the provided information. Stick strictly to what is directly supported.\n",
    "\n",
    "Provide only the letter (A, B, C, D, or E) corresponding to the single best and most evidence-backed option.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUeY3w3VVS9U"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load mention vectors from our knowledge source\n",
    "    with open(\"optionll_embeddings_and_sentences.json\") as f:\n",
    "        mention_data = json.load(f)\n",
    "\n",
    "    # Normalize mention keys\n",
    "    normalized_mention_data = {\n",
    "        normalize(k): v for k, v in mention_data.items()\n",
    "    }\n",
    "\n",
    "    # Load MEDQA dataset\n",
    "    with open(\"medqal.jsonl\") as f:\n",
    "        medqa_data = [json.loads(line) for line in f]\n",
    "\n",
    "    #define y_true and y_pred (used for Evaluation)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    total = 0\n",
    "\n",
    "    #looping through Medqa Questions\n",
    "    print(\"Running QA loop...\")\n",
    "    for idx, sample in enumerate(tqdm(medqa_data, desc=\"Evaluating\")):\n",
    "        question = sample[\"question\"]\n",
    "        option_dict = sample[\"options\"]  # dict: {'A': ..., 'B': ..., ...}\n",
    "        options = [option_dict[k] for k in sorted(option_dict.keys())]  # Keep order: A-E\n",
    "        correct_idx = options.index(sample[\"answer\"]) #we take it from the medqa file\n",
    "        total += 1\n",
    "\n",
    "        # Embed question using your existing function\n",
    "        q_vec = embed_text(question, tokenizer, bert_model)\n",
    "\n",
    "        # Similarity scoring\n",
    "        scored = []\n",
    "        for opt in options:\n",
    "            opt_norm = normalize(opt)\n",
    "            # print(f'Normalized Option: {opt_norm}')\n",
    "            #checking for the availability of our option\n",
    "            if opt_norm not in normalized_mention_data:\n",
    "                print(f\"[WARN] Option not found in mention_data: '{opt}'\")\n",
    "                scored.append((opt, -1.0, []))\n",
    "                continue\n",
    "\n",
    "            data = normalized_mention_data[opt_norm]\n",
    "\n",
    "            # calculating the cosine similarity between question and the avg emb. vector of the options\n",
    "            sim = cosine_sim(q_vec, data[\"avg_vector\"])\n",
    "            \n",
    "            context = data[\"top_sentences\"][:5]\n",
    "            # add each option and it's score + context sentences\n",
    "            scored.append((opt, sim, context))\n",
    "\n",
    "        # Sort by similarity\n",
    "        top_options = sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "        #top1 and top2 are the top scoring options\n",
    "        top1, top2, top3, top4, top5 = top_options[0], top_options[1], top_options[2], top_options[3], top_options[4]\n",
    "\n",
    "        #prints used for debugging (not important)\n",
    "        # print('*'*10)\n",
    "        # print(f\"Top1: {top1}\\n\")\n",
    "        # print(f\"Top2: {top2}\\n\")\n",
    "        # print(f\"Top3: {top3}\\n\")\n",
    "        # print(f\"Top4: {top4}\\n\")\n",
    "        # print(f\"Top5: {top5}\\n\")\n",
    "        # print('*'*10)\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        #Building Prompt\n",
    "        prompt = build_prompt(question, options, top1, top2, top3, top4, top5)\n",
    "        # print(f\"Prompt:\\n{prompt}\")\n",
    "        # print('*'*10)\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        #generating output\n",
    "        output = query_llm(llm_pipe, prompt)\n",
    "        # print(output)\n",
    "        # print('*'*10)\n",
    "        #extracting the answer from the generated output\n",
    "        pred_idx = extract_answer(output, options)\n",
    "        print(f\"Predicted Index/Answer: {pred_idx}\")\n",
    "        print('*'*10)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        #---------------------------------------------------------\n",
    "\n",
    "        #append the correct answer and the predicted answer\n",
    "        y_true.append(correct_idx)\n",
    "        y_pred.append(pred_idx)\n",
    "\n",
    "        # Print answer for each question\n",
    "        print(f\"\\n--- Question {idx + 1} ---\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Options: {options}\")\n",
    "        print(f\"LLM Output: {output}\")\n",
    "        if pred_idx != -1:\n",
    "            print(f\"Predicted Answer: {chr(65 + pred_idx)}. {options[pred_idx]}\")\n",
    "        else:\n",
    "            print(\"Predicted Answer: Invalid or Unrecognized\")\n",
    "        print(f\"Correct Answer: {chr(65 + correct_idx)}. {options[correct_idx]}\")\n",
    "        print('*--*'*100)\n",
    "\n",
    "    # Evaluation (accuracy + F1_score)\n",
    "    correct = [yt == yp for yt, yp in zip(y_true, y_pred)]\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_micro = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    print(f\"Total Questions: {total}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"F1 Score (micro): {f1_micro:.2%}\")\n",
    "    print(f\"F1 Score (macro): {f1_macro:.2%}\")\n",
    "    print(f\"Invalid Predictions: {sum(p == -1 for p in y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwo426TRVS61"
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPPV9Ym5/IiRQCm91OqVV6V",
   "collapsed_sections": [
    "87L-zQu3BXT4",
    "qf02MV-tBdtn",
    "GHHKdDWrByYK",
    "-v-f6reWB5m4",
    "gaNWGxVgCGqN",
    "w5bDAtXyCLn2",
    "8i4iKETMCZJl",
    "CNlX9fSqD1QI",
    "o5MpPGhwThLr",
    "60aWNRW2CtTz",
    "usNanvpkC1aM",
    "BOToX0vxR0aG",
    "WSHUs5rmUIxg",
    "qbH0a-2PUzHq",
    "c7fnm4M7ZJgM",
    "yit_opPBWpPO"
   ],
   "provenance": [
    {
     "file_id": "1OVBfhUQ7dLX03k2JoPN9A6S6x89Zr1YH",
     "timestamp": 1753099465993
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
